{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bita049b6f31a414bba88280c3b8f9cd974",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "import json\n",
    "import os\n",
    "import quaternion\n",
    "import h5py\n",
    "import models\n",
    "\n",
    "#import capsulelayers as capslayers\n",
    "#import capsulenet as capsnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess JSON data and create the dataset\n",
    "training_dataset_folder_path = \"C:/Users/BORA/Desktop/lm_test_no_occlusion\" #change this\n",
    "training_split = \"train\"\n",
    "output_classes = 3 # Can vary depending on the amount of objects\n",
    "\n",
    "# Save the labels. \n",
    "object_ids = []\n",
    "poses      = []\n",
    "bboxes     = []\n",
    "\n",
    "for scene in os.listdir(os.path.join(training_dataset_folder_path, training_split)):\n",
    "    scene_gt_path      = os.path.join(training_dataset_folder_path, training_split, scene, \"scene_gt.json\")\n",
    "    scene_gt_info_path = os.path.join(training_dataset_folder_path, training_split, scene, \"scene_gt_info.json\")\n",
    "    \n",
    "    with open(scene_gt_path, 'r') as scene_gt_file:\n",
    "        scene_gt_json = json.load(scene_gt_file)\n",
    "        for img_gt_key in scene_gt_json:\n",
    "            translation = scene_gt_json[img_gt_key][0]['cam_t_m2c']\n",
    "            rotation    = scene_gt_json[img_gt_key][0]['cam_R_m2c']\n",
    "            rotation    = np.reshape(rotation, (3,3))\n",
    "            rotation    = quaternion.from_rotation_matrix(rotation)\n",
    "            rotation    = quaternion.as_float_array(rotation)\n",
    "            obj_id      = scene_gt_json[img_gt_key][0]['obj_id']\n",
    "\n",
    "            label_id     = []   # One-hot encoding\n",
    "            label_pose   = []   # Transform + Quaternion\n",
    "\n",
    "            for index in range(output_classes):\n",
    "                if index==obj_id-1:\n",
    "                    label_id.append(1.0)\n",
    "                else:\n",
    "                    label_id.append(0.0)\n",
    "            label_id = np.asarray(label_id)\n",
    "            object_ids.append(label_id)\n",
    "\n",
    "            label_pose.append(translation[0])\n",
    "            label_pose.append(translation[1])\n",
    "            label_pose.append(translation[2])\n",
    "            label_pose.append(rotation[0])\n",
    "            label_pose.append(rotation[1])\n",
    "            label_pose.append(rotation[2])\n",
    "            label_pose.append(rotation[3])\n",
    "\n",
    "            label_pose = np.asarray(label_pose)\n",
    "            poses.append(label_pose)\n",
    "            \n",
    "\n",
    "    with open(scene_gt_info_path, 'r') as scene_gt_info_file:\n",
    "        scene_gt_info_json = json.load(scene_gt_info_file)\n",
    "        for img_gt_key in scene_gt_info_json:\n",
    "            labels_bboxes = [] # BBox coords\n",
    "            bbox = scene_gt_info_json[img_gt_key][0]['bbox_obj']\n",
    "            labels_bboxes.append(bbox[0])\n",
    "            labels_bboxes.append(bbox[1])\n",
    "            labels_bboxes.append(bbox[2])\n",
    "            labels_bboxes.append(bbox[3])\n",
    "\n",
    "            labels_bboxes = np.asarray(labels_bboxes)\n",
    "            bboxes.append(labels_bboxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1. 0. 0.]\n[ 0.00000000e+00  0.00000000e+00  9.93988500e+02  2.44412794e-01\n -1.13855420e-02  9.68554121e-01  4.51184035e-02]\n[ 255  153  129 -187]\n"
    }
   ],
   "source": [
    "print(object_ids[0])\n",
    "print(poses[0])\n",
    "print(bboxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(224, 224, 3)\n"
    }
   ],
   "source": [
    "# Make a dataset from the training images\n",
    "\n",
    "training_images = []\n",
    "i = 0\n",
    "for scene in os.listdir(os.path.join(training_dataset_folder_path, training_split)):\n",
    "    rgb_images_path = os.path.join(training_dataset_folder_path, training_split, scene, \"rgb\")\n",
    "\n",
    "    for img in os.listdir(rgb_images_path):\n",
    "        # Open the image file\n",
    "        img = tf.io.read_file(os.path.join(rgb_images_path, img))\n",
    "        # convert the compressed string to a 3D uint8 tensor\n",
    "        img = tf.image.decode_png(img, channels=3)\n",
    "        # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        # resize the image to the desired size.\n",
    "        img = tf.image.resize(img, [224, 224])\n",
    "        # add to the image dataset\n",
    "        training_images.append(img.numpy())\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input': training_images}, {'class_output': object_ids, 'pose_output': poses, 'bbox_output': bboxes}))\n",
    "train_dataset = train_dataset.batch(1)\n",
    "print(training_images[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<BatchDataset shapes: ({input: (None, 224, 224, 3)}, {class_output: (None, 3), pose_output: (None, 7), bbox_output: (None, 4)}), types: ({input: tf.float32}, {class_output: tf.float64, pose_output: tf.float64, bbox_output: tf.int32})>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "________________________________________________________________________________________________\nconv2d_47 (Conv2D)              (None, 23, 23, 32)   25120       dropout_15[0][0]                 \n__________________________________________________________________________________________________\nconv2d_48 (Conv2D)              (None, 10, 10, 16)   12816       conv2d_47[0][0]                  \n__________________________________________________________________________________________________\nconv2d_49 (Conv2D)              (None, 8, 8, 8)      1160        conv2d_48[0][0]                  \n__________________________________________________________________________________________________\nconv2d_50 (Conv2D)              (None, 6, 6, 16)     1168        conv2d_49[0][0]                  \n__________________________________________________________________________________________________\ndropout_16 (Dropout)            (None, 6, 6, 16)     0           conv2d_50[0][0]                  \n__________________________________________________________________________________________________\nconv2d_51 (Conv2D)              (None, 4, 4, 32)     4640        dropout_16[0][0]                 \n__________________________________________________________________________________________________\nconv2d_52 (Conv2D)              (None, 2, 2, 16)     4624        conv2d_51[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_5 (UpSampling2D)  (None, 96, 96, 16)   0           conv2d_52[0][0]                  \n__________________________________________________________________________________________________\nconv2d_53 (Conv2D)              (None, 94, 94, 3)    435         up_sampling2d_5[0][0]            \n__________________________________________________________________________________________________\nflatten_11 (Flatten)            (None, 26508)        0           conv2d_53[0][0]                  \n__________________________________________________________________________________________________\nflatten_10 (Flatten)            (None, 1600)         0           conv2d_48[0][0]                  \n__________________________________________________________________________________________________\ndense_44 (Dense)                (None, 1024)         27145216    flatten_11[0][0]                 \n__________________________________________________________________________________________________\ndense_40 (Dense)                (None, 1024)         1639424     flatten_10[0][0]                 \n__________________________________________________________________________________________________\ndense_45 (Dense)                (None, 512)          524800      dense_44[0][0]                   \n__________________________________________________________________________________________________\ndense_41 (Dense)                (None, 512)          524800      dense_40[0][0]                   \n__________________________________________________________________________________________________\ndense_46 (Dense)                (None, 256)          131328      dense_45[0][0]                   \n__________________________________________________________________________________________________\ndense_42 (Dense)                (None, 256)          131328      dense_41[0][0]                   \n__________________________________________________________________________________________________\ndense_47 (Dense)                (None, 128)          32896       dense_46[0][0]                   \n__________________________________________________________________________________________________\nmobilenetv2_1.00_224 (Model)    (None, 3)            2261827     conv2d_53[0][0]                  \n__________________________________________________________________________________________________\ndense_43 (Dense)                (None, 128)          32896       dense_42[0][0]                   \n__________________________________________________________________________________________________\ndropout_17 (Dropout)            (None, 128)          0           dense_47[0][0]                   \n__________________________________________________________________________________________________\nclass_output (Dense)            (None, 3)            12          mobilenetv2_1.00_224[1][0]       \n__________________________________________________________________________________________________\npose_output (Dense)             (None, 7)            903         dense_43[0][0]                   \n__________________________________________________________________________________________________\nbbox_output (Dense)             (None, 4)            516         dropout_17[0][0]                 \n==================================================================================================\nTotal params: 32,483,381\nTrainable params: 32,449,269\nNon-trainable params: 34,112\n__________________________________________________________________________________________________\nWARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\nEpoch 1/50\n3000/3000 [==============================] - 250s 83ms/step - loss: 22988.0195 - class_output_loss: 1.0331 - pose_output_loss: 11371.9805 - bbox_output_loss: 11615.0049 - class_output_categorical_accuracy: 0.3643 - pose_output_mean_squared_error: 11371.9805 - bbox_output_mean_squared_error: 11615.0049\nEpoch 2/50\n3000/3000 [==============================] - 218s 73ms/step - loss: 17915.0421 - class_output_loss: 1.1229 - pose_output_loss: 6940.4028 - bbox_output_loss: 10969.6709 - class_output_categorical_accuracy: 0.4330 - pose_output_mean_squared_error: 6940.4028 - bbox_output_mean_squared_error: 10969.6709\nEpoch 3/50\n3000/3000 [==============================] - 225s 75ms/step - loss: 16410.5277 - class_output_loss: 1.1043 - pose_output_loss: 5527.6309 - bbox_output_loss: 10878.4111 - class_output_categorical_accuracy: 0.2223 - pose_output_mean_squared_error: 5527.6309 - bbox_output_mean_squared_error: 10878.4111\nEpoch 4/50\n3000/3000 [==============================] - 221s 74ms/step - loss: 15166.0752 - class_output_loss: 1.0035 - pose_output_loss: 4287.4473 - bbox_output_loss: 10874.7930 - class_output_categorical_accuracy: 0.4393 - pose_output_mean_squared_error: 4287.4473 - bbox_output_mean_squared_error: 10874.7930\nEpoch 5/50\n2999/3000 [============================>.] - ETA: 0s - loss: 14440.4096 - class_output_loss: 0.9929 - pose_output_loss: 3611.1997 - bbox_output_loss: 10828.2041 - class_output_categorical_accuracy: 0.3404 - pose_output_mean_squared_error: 3611.1997 - bbox_output_mean_squared_error: 10828.2041\nEpoch 00005: saving model to mobile_pose0005.h5\n3000/3000 [==============================] - 222s 74ms/step - loss: 14440.4087 - class_output_loss: 0.9927 - pose_output_loss: 3610.0093 - bbox_output_loss: 10826.6680 - class_output_categorical_accuracy: 0.3407 - pose_output_mean_squared_error: 3610.0093 - bbox_output_mean_squared_error: 10826.6680\nEpoch 6/50\n3000/3000 [==============================] - 220s 73ms/step - loss: 13900.6938 - class_output_loss: 0.9676 - pose_output_loss: 3115.0508 - bbox_output_loss: 10782.2490 - class_output_categorical_accuracy: 0.4757 - pose_output_mean_squared_error: 3115.0508 - bbox_output_mean_squared_error: 10782.2490\nEpoch 7/50\n3000/3000 [==============================] - 220s 73ms/step - loss: 13287.3924 - class_output_loss: 1.0381 - pose_output_loss: 2560.8096 - bbox_output_loss: 10723.6289 - class_output_categorical_accuracy: 0.5557 - pose_output_mean_squared_error: 2560.8096 - bbox_output_mean_squared_error: 10723.6289\nEpoch 8/50\n3000/3000 [==============================] - 217s 72ms/step - loss: 12773.6501 - class_output_loss: 1.0229 - pose_output_loss: 2097.3853 - bbox_output_loss: 10673.2637 - class_output_categorical_accuracy: 0.4947 - pose_output_mean_squared_error: 2097.3853 - bbox_output_mean_squared_error: 10673.2637\nEpoch 9/50\n3000/3000 [==============================] - 220s 73ms/step - loss: 12504.9768 - class_output_loss: 1.0849 - pose_output_loss: 1847.5155 - bbox_output_loss: 10654.5264 - class_output_categorical_accuracy: 0.4753 - pose_output_mean_squared_error: 1847.5155 - bbox_output_mean_squared_error: 10654.5264\nEpoch 10/50\n2999/3000 [============================>.] - ETA: 0s - loss: 12162.6162 - class_output_loss: 0.9914 - pose_output_loss: 1516.3417 - bbox_output_loss: 10645.2812 - class_output_categorical_accuracy: 0.6182 - pose_output_mean_squared_error: 1516.3417 - bbox_output_mean_squared_error: 10645.2812\nEpoch 00010: saving model to mobile_pose0010.h5\n3000/3000 [==============================] - 223s 74ms/step - loss: 12162.6156 - class_output_loss: 0.9913 - pose_output_loss: 1516.1532 - bbox_output_loss: 10643.8271 - class_output_categorical_accuracy: 0.6183 - pose_output_mean_squared_error: 1516.1532 - bbox_output_mean_squared_error: 10643.8271\nEpoch 11/50\n3000/3000 [==============================] - 219s 73ms/step - loss: 11867.8318 - class_output_loss: 0.8873 - pose_output_loss: 1237.3945 - bbox_output_loss: 10627.9404 - class_output_categorical_accuracy: 0.7280 - pose_output_mean_squared_error: 1237.3945 - bbox_output_mean_squared_error: 10627.9404\nEpoch 12/50\n3000/3000 [==============================] - 223s 74ms/step - loss: 11623.2272 - class_output_loss: 0.7809 - pose_output_loss: 1004.3100 - bbox_output_loss: 10616.5195 - class_output_categorical_accuracy: 0.7813 - pose_output_mean_squared_error: 1004.3100 - bbox_output_mean_squared_error: 10616.5195\nEpoch 13/50\n3000/3000 [==============================] - 221s 74ms/step - loss: 11548.9759 - class_output_loss: 0.7548 - pose_output_loss: 941.5818 - bbox_output_loss: 10605.2451 - class_output_categorical_accuracy: 0.7670 - pose_output_mean_squared_error: 941.5818 - bbox_output_mean_squared_error: 10605.2451\nEpoch 14/50\n3000/3000 [==============================] - 218s 73ms/step - loss: 11408.2758 - class_output_loss: 0.7034 - pose_output_loss: 821.3957 - bbox_output_loss: 10584.7109 - class_output_categorical_accuracy: 0.7787 - pose_output_mean_squared_error: 821.3957 - bbox_output_mean_squared_error: 10584.7109\nEpoch 15/50\n2999/3000 [============================>.] - ETA: 0s - loss: 11331.5569 - class_output_loss: 0.6581 - pose_output_loss: 739.3404 - bbox_output_loss: 10591.5596 - class_output_categorical_accuracy: 0.7919 - pose_output_mean_squared_error: 739.3404 - bbox_output_mean_squared_error: 10591.5596\nEpoch 00015: saving model to mobile_pose0015.h5\n3000/3000 [==============================] - 225s 75ms/step - loss: 11331.5565 - class_output_loss: 0.6580 - pose_output_loss: 739.4174 - bbox_output_loss: 10590.1064 - class_output_categorical_accuracy: 0.7920 - pose_output_mean_squared_error: 739.4174 - bbox_output_mean_squared_error: 10590.1064\nEpoch 16/50\n3000/3000 [==============================] - 222s 74ms/step - loss: 11234.1576 - class_output_loss: 0.5997 - pose_output_loss: 664.9562 - bbox_output_loss: 10567.1465 - class_output_categorical_accuracy: 0.8127 - pose_output_mean_squared_error: 664.9562 - bbox_output_mean_squared_error: 10567.1465\nEpoch 17/50\n3000/3000 [==============================] - 218s 73ms/step - loss: 11129.9769 - class_output_loss: 0.5350 - pose_output_loss: 559.3201 - bbox_output_loss: 10568.7070 - class_output_categorical_accuracy: 0.8420 - pose_output_mean_squared_error: 559.3201 - bbox_output_mean_squared_error: 10568.7070\nEpoch 18/50\n3000/3000 [==============================] - 222s 74ms/step - loss: 11074.3941 - class_output_loss: 0.5341 - pose_output_loss: 524.8768 - bbox_output_loss: 10547.6094 - class_output_categorical_accuracy: 0.8353 - pose_output_mean_squared_error: 524.8768 - bbox_output_mean_squared_error: 10547.6094\nEpoch 19/50\n3000/3000 [==============================] - 221s 74ms/step - loss: 11007.6476 - class_output_loss: 0.4982 - pose_output_loss: 462.8532 - bbox_output_loss: 10543.3037 - class_output_categorical_accuracy: 0.8460 - pose_output_mean_squared_error: 462.8532 - bbox_output_mean_squared_error: 10543.3037\nEpoch 20/50\n2999/3000 [============================>.] - ETA: 0s - loss: 11017.5839 - class_output_loss: 0.4603 - pose_output_loss: 472.9010 - bbox_output_loss: 10544.2266 - class_output_categorical_accuracy: 0.8616 - pose_output_mean_squared_error: 472.9010 - bbox_output_mean_squared_error: 10544.2266\nEpoch 00020: saving model to mobile_pose0020.h5\n3000/3000 [==============================] - 232s 77ms/step - loss: 11017.5834 - class_output_loss: 0.4602 - pose_output_loss: 472.9251 - bbox_output_loss: 10542.7803 - class_output_categorical_accuracy: 0.8617 - pose_output_mean_squared_error: 472.9251 - bbox_output_mean_squared_error: 10542.7803\nEpoch 21/50\n3000/3000 [==============================] - 235s 78ms/step - loss: 11032.5028 - class_output_loss: 0.4355 - pose_output_loss: 496.7223 - bbox_output_loss: 10533.7695 - class_output_categorical_accuracy: 0.8697 - pose_output_mean_squared_error: 496.7223 - bbox_output_mean_squared_error: 10533.7695\nEpoch 22/50\n3000/3000 [==============================] - 212s 71ms/step - loss: 10939.1993 - class_output_loss: 0.3928 - pose_output_loss: 409.8814 - bbox_output_loss: 10527.3506 - class_output_categorical_accuracy: 0.8857 - pose_output_mean_squared_error: 409.8814 - bbox_output_mean_squared_error: 10527.3506\nEpoch 23/50\n3000/3000 [==============================] - 214s 71ms/step - loss: 10911.2323 - class_output_loss: 0.3717 - pose_output_loss: 388.3899 - bbox_output_loss: 10520.9082 - class_output_categorical_accuracy: 0.8937 - pose_output_mean_squared_error: 388.3899 - bbox_output_mean_squared_error: 10520.9082\nEpoch 24/50\n3000/3000 [==============================] - 209s 70ms/step - loss: 10898.3698 - class_output_loss: 0.3662 - pose_output_loss: 381.1747 - bbox_output_loss: 10515.2529 - class_output_categorical_accuracy: 0.8940 - pose_output_mean_squared_error: 381.1747 - bbox_output_mean_squared_error: 10515.2529\nEpoch 25/50\n2999/3000 [============================>.] - ETA: 0s - loss: 10859.8023 - class_output_loss: 0.3396 - pose_output_loss: 341.6377 - bbox_output_loss: 10517.8262 - class_output_categorical_accuracy: 0.9036 - pose_output_mean_squared_error: 341.6377 - bbox_output_mean_squared_error: 10517.8262\nEpoch 00025: saving model to mobile_pose0025.h5\n3000/3000 [==============================] - 216s 72ms/step - loss: 10859.8018 - class_output_loss: 0.3395 - pose_output_loss: 341.5240 - bbox_output_loss: 10516.5244 - class_output_categorical_accuracy: 0.9037 - pose_output_mean_squared_error: 341.5240 - bbox_output_mean_squared_error: 10516.5244\nEpoch 26/50\n3000/3000 [==============================] - 219s 73ms/step - loss: 10837.3425 - class_output_loss: 0.3080 - pose_output_loss: 332.2515 - bbox_output_loss: 10503.1592 - class_output_categorical_accuracy: 0.9150 - pose_output_mean_squared_error: 332.2515 - bbox_output_mean_squared_error: 10503.1592\nEpoch 27/50\n3000/3000 [==============================] - 212s 71ms/step - loss: 10818.0086 - class_output_loss: 0.2847 - pose_output_loss: 324.9376 - bbox_output_loss: 10491.1992 - class_output_categorical_accuracy: 0.9237 - pose_output_mean_squared_error: 324.9376 - bbox_output_mean_squared_error: 10491.1992\nEpoch 28/50\n3000/3000 [==============================] - 219s 73ms/step - loss: 10823.3381 - class_output_loss: 0.2824 - pose_output_loss: 328.9157 - bbox_output_loss: 10492.5498 - class_output_categorical_accuracy: 0.9240 - pose_output_mean_squared_error: 328.9157 - bbox_output_mean_squared_error: 10492.5498\nEpoch 29/50\n3000/3000 [==============================] - 216s 72ms/step - loss: 10806.1457 - class_output_loss: 0.2780 - pose_output_loss: 304.9840 - bbox_output_loss: 10499.4658 - class_output_categorical_accuracy: 0.9253 - pose_output_mean_squared_error: 304.9840 - bbox_output_mean_squared_error: 10499.4658\nEpoch 30/50\n2999/3000 [============================>.] - ETA: 0s - loss: 10775.9695 - class_output_loss: 0.2599 - pose_output_loss: 290.7857 - bbox_output_loss: 10484.9189 - class_output_categorical_accuracy: 0.9310 - pose_output_mean_squared_error: 290.7857 - bbox_output_mean_squared_error: 10484.9189\nEpoch 00030: saving model to mobile_pose0030.h5\n3000/3000 [==============================] - 191s 64ms/step - loss: 10775.9689 - class_output_loss: 0.2599 - pose_output_loss: 290.6992 - bbox_output_loss: 10483.3838 - class_output_categorical_accuracy: 0.9310 - pose_output_mean_squared_error: 290.6992 - bbox_output_mean_squared_error: 10483.3838\nEpoch 31/50\n3000/3000 [==============================] - 213s 71ms/step - loss: 10776.1441 - class_output_loss: 0.2613 - pose_output_loss: 299.7932 - bbox_output_loss: 10474.5186 - class_output_categorical_accuracy: 0.9307 - pose_output_mean_squared_error: 299.7932 - bbox_output_mean_squared_error: 10474.5186\nEpoch 32/50\n3000/3000 [==============================] - 235s 78ms/step - loss: 10743.1114 - class_output_loss: 0.2403 - pose_output_loss: 269.6494 - bbox_output_loss: 10471.6250 - class_output_categorical_accuracy: 0.9377 - pose_output_mean_squared_error: 269.6494 - bbox_output_mean_squared_error: 10471.6250\nEpoch 33/50\n3000/3000 [==============================] - 257s 86ms/step - loss: 10735.5015 - class_output_loss: 0.2335 - pose_output_loss: 260.2025 - bbox_output_loss: 10473.5664 - class_output_categorical_accuracy: 0.9400 - pose_output_mean_squared_error: 260.2025 - bbox_output_mean_squared_error: 10473.5664\nEpoch 34/50\n3000/3000 [==============================] - 233s 78ms/step - loss: 10721.0227 - class_output_loss: 0.2178 - pose_output_loss: 242.7186 - bbox_output_loss: 10476.5918 - class_output_categorical_accuracy: 0.9450 - pose_output_mean_squared_error: 242.7186 - bbox_output_mean_squared_error: 10476.5918\nEpoch 35/50\n2999/3000 [============================>.] - ETA: 0s - loss: 10681.5160 - class_output_loss: 0.2211 - pose_output_loss: 213.3656 - bbox_output_loss: 10467.9404 - class_output_categorical_accuracy: 0.9440 - pose_output_mean_squared_error: 213.3656 - bbox_output_mean_squared_error: 10467.9404\nEpoch 00035: saving model to mobile_pose0035.h5\n3000/3000 [==============================] - 221s 74ms/step - loss: 10681.5155 - class_output_loss: 0.2211 - pose_output_loss: 213.2959 - bbox_output_loss: 10466.4443 - class_output_categorical_accuracy: 0.9440 - pose_output_mean_squared_error: 213.2959 - bbox_output_mean_squared_error: 10466.4443\nEpoch 36/50\n3000/3000 [==============================] - 217s 72ms/step - loss: 10659.7276 - class_output_loss: 0.2126 - pose_output_loss: 196.7951 - bbox_output_loss: 10461.2754 - class_output_categorical_accuracy: 0.9467 - pose_output_mean_squared_error: 196.7951 - bbox_output_mean_squared_error: 10461.2754\nEpoch 37/50\n3000/3000 [==============================] - 220s 73ms/step - loss: 10633.7796 - class_output_loss: 0.1983 - pose_output_loss: 173.8852 - bbox_output_loss: 10458.3965 - class_output_categorical_accuracy: 0.9513 - pose_output_mean_squared_error: 173.8852 - bbox_output_mean_squared_error: 10458.3965\nEpoch 38/50\n3000/3000 [==============================] - 220s 73ms/step - loss: 10617.6689 - class_output_loss: 0.2042 - pose_output_loss: 161.0690 - bbox_output_loss: 10454.8848 - class_output_categorical_accuracy: 0.9493 - pose_output_mean_squared_error: 161.0690 - bbox_output_mean_squared_error: 10454.8848\nEpoch 39/50\n3000/3000 [==============================] - 216s 72ms/step - loss: 10617.3801 - class_output_loss: 0.1964 - pose_output_loss: 166.7853 - bbox_output_loss: 10448.9756 - class_output_categorical_accuracy: 0.9517 - pose_output_mean_squared_error: 166.7853 - bbox_output_mean_squared_error: 10448.9756\nEpoch 40/50\n2236/3000 [=====================>........] - ETA: 58s - loss: 11683.6701 - class_output_loss: 0.2407 - pose_output_loss: 199.8029 - bbox_output_loss: 11483.6250 - class_output_categorical_accuracy: 0.9383 - pose_output_mean_squared_error: 199.8029 - bbox_output_mean_squared_error: 11483.6250"
    }
   ],
   "source": [
    "#Training\n",
    "model = MobilePoseNet()\n",
    "model.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "tf.keras.utils.plot_model(model, 'model.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "\n",
    "checkpoint_path = \"mobile_pose{epoch:04d}.h5\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=1, period=5)\n",
    "\n",
    "from datetime import datetime\n",
    "logdir = \"logs\\\\scalars\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "early_stopping_callback_class = tf.keras.callbacks.EarlyStopping(monitor='class_output_categorical_accuracy', patience=10, verbose=1, mode='max')\n",
    "early_stopping_callback_pose  = tf.keras.callbacks.EarlyStopping(monitor='pose_output_mean_squared_error', patience=10, verbose=1, mode='min')\n",
    "early_stopping_callback_bbox  = tf.keras.callbacks.EarlyStopping(monitor='bbox_output_mean_squared_error', patience=10, verbose=1, mode='min')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=[tf.keras.losses.CategoricalCrossentropy(), \n",
    "                                         tf.keras.losses.MeanSquaredError(), \n",
    "                                         tf.keras.losses.MeanSquaredError()], \n",
    "                                   metrics={'class_output': [tf.keras.metrics.CategoricalAccuracy()],\n",
    "                                            'pose_output': [tf.keras.metrics.MeanSquaredError()],\n",
    "                                            'bbox_output': [tf.keras.metrics.MeanSquaredError()]})\n",
    "\n",
    "history = model.fit(train_dataset, epochs=50, use_multiprocessing=True, callbacks=[cp_callback, tensorboard_callback, early_stopping_callback_bbox,\n",
    "                                                                                    early_stopping_callback_class, early_stopping_callback_pose])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}