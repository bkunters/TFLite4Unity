{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bita049b6f31a414bba88280c3b8f9cd974",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Python37\\lib\\site-packages\\quaternion\\numba_wrapper.py:20: UserWarning: \n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nCould not import from numba, which means that some\nparts of this code may run MUCH more slowly.  You\nmay wish to install numba.\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n  warnings.warn(warning_text)\nC:\\Python37\\lib\\site-packages\\quaternion\\calculus.py:475: UserWarning: \n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nCould not import from scipy, which means that derivatives\nand integrals will use less accurate finite-differencing\ntechniques.  You may want to install scipy.\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n  warnings.warn(warning_text)\n"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "import json\n",
    "import os\n",
    "import quaternion\n",
    "import h5py\n",
    "import models\n",
    "\n",
    "#import capsulelayers as capslayers\n",
    "#import capsulenet as capsnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess JSON data and create the dataset\n",
    "# TODO: set as argument\n",
    "test_dataset_folder_path     = \"C:/Users/BORA/Desktop/lm_test_bop19\"\n",
    "test_split                   = \"test\"\n",
    "output_classes = 3 # Can vary depending on the amount of objects\n",
    "\n",
    "# Save the labels. \n",
    "object_ids = []\n",
    "poses      = []\n",
    "bboxes     = []\n",
    "\n",
    "for scene in os.listdir(os.path.join(test_dataset_folder_path, test_split)):\n",
    "    scene_gt_path      = os.path.join(test_dataset_folder_path, test_split, scene, \"scene_gt.json\")\n",
    "    scene_gt_info_path = os.path.join(test_dataset_folder_path, test_split, scene, \"scene_gt_info.json\")\n",
    "    \n",
    "    with open(scene_gt_path, 'r') as scene_gt_file:\n",
    "        scene_gt_json = json.load(scene_gt_file)\n",
    "        for img_gt_key in scene_gt_json:\n",
    "            translation = scene_gt_json[img_gt_key][0]['cam_t_m2c']\n",
    "            rotation    = scene_gt_json[img_gt_key][0]['cam_R_m2c']\n",
    "            rotation    = np.reshape(rotation, (3,3))\n",
    "            rotation    = quaternion.from_rotation_matrix(rotation)\n",
    "            rotation    = quaternion.as_float_array(rotation)\n",
    "            obj_id      = scene_gt_json[img_gt_key][0]['obj_id']\n",
    "\n",
    "            label_id     = []   # One-hot encoding\n",
    "            label_pose   = []   # Transform + Quaternion\n",
    "\n",
    "            for index in range(output_classes):\n",
    "                if index==obj_id-1: # todo: must be -1 later!\n",
    "                    label_id.append(1.0)\n",
    "                else:\n",
    "                    label_id.append(0.0)\n",
    "            label_id = np.asarray(label_id)\n",
    "            object_ids.append(label_id)\n",
    "\n",
    "            label_pose.append(translation[0])\n",
    "            label_pose.append(translation[1])\n",
    "            label_pose.append(translation[2])\n",
    "            label_pose.append(rotation[0])\n",
    "            label_pose.append(rotation[1])\n",
    "            label_pose.append(rotation[2])\n",
    "            label_pose.append(rotation[3])\n",
    "\n",
    "            label_pose = np.asarray(label_pose)\n",
    "            poses.append(label_pose)\n",
    "            \n",
    "\n",
    "    with open(scene_gt_info_path, 'r') as scene_gt_info_file:\n",
    "        scene_gt_info_json = json.load(scene_gt_info_file)\n",
    "        for img_gt_key in scene_gt_info_json:\n",
    "            labels_bboxes = [] # BBox coords\n",
    "            bbox = scene_gt_info_json[img_gt_key][0]['bbox_obj']\n",
    "            labels_bboxes.append(bbox[0])\n",
    "            labels_bboxes.append(bbox[1])\n",
    "            labels_bboxes.append(bbox[2])\n",
    "            labels_bboxes.append(bbox[3])\n",
    "\n",
    "            labels_bboxes = np.asarray(labels_bboxes)\n",
    "            bboxes.append(labels_bboxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(224, 224, 3)\n"
    }
   ],
   "source": [
    "test_images = []\n",
    "i = 0\n",
    "for scene in os.listdir(os.path.join(test_dataset_folder_path, test_split)):\n",
    "    rgb_images_path = os.path.join(test_dataset_folder_path, test_split, scene, \"rgb\")\n",
    "\n",
    "    for img in os.listdir(rgb_images_path):\n",
    "        # Open the image file\n",
    "        img = tf.io.read_file(os.path.join(rgb_images_path, img))\n",
    "        # convert the compressed string to a 3D uint8 tensor\n",
    "        img = tf.image.decode_png(img, channels=3)\n",
    "        # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        # resize the image to the desired size.\n",
    "        img = tf.image.resize(img, [224, 224])\n",
    "        # add to the image dataset\n",
    "        test_images.append(img.numpy())\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(({'input': test_images}, {'class_output': object_ids, 'pose_output': poses, 'bbox_output': bboxes}))\n",
    "test_dataset = test_dataset.batch(1)\n",
    "print(test_images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<BatchDataset shapes: ({input: (None, 224, 224, 3)}, {class_output: (None, 3), pose_output: (None, 7), bbox_output: (None, 4)}), types: ({input: tf.float32}, {class_output: tf.float64, pose_output: tf.float64, bbox_output: tf.int32})>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model_9\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput (InputLayer)              [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d_81 (Conv2D)              (None, 109, 109, 8)  1184        input[0][0]                      \n__________________________________________________________________________________________________\nconv2d_82 (Conv2D)              (None, 52, 52, 16)   6288        conv2d_81[0][0]                  \n__________________________________________________________________________________________________\ndropout_20 (Dropout)            (None, 52, 52, 16)   0           conv2d_82[0][0]                  \n__________________________________________________________________________________________________\nconv2d_83 (Conv2D)              (None, 23, 23, 32)   25120       dropout_20[0][0]                 \n__________________________________________________________________________________________________\nconv2d_84 (Conv2D)              (None, 10, 10, 16)   12816       conv2d_83[0][0]                  \n__________________________________________________________________________________________________\nconv2d_85 (Conv2D)              (None, 8, 8, 8)      1160        conv2d_84[0][0]                  \n__________________________________________________________________________________________________\nconv2d_86 (Conv2D)              (None, 6, 6, 16)     1168        conv2d_85[0][0]                  \n__________________________________________________________________________________________________\ndropout_21 (Dropout)            (None, 6, 6, 16)     0           conv2d_86[0][0]                  \n__________________________________________________________________________________________________\nconv2d_87 (Conv2D)              (None, 4, 4, 32)     4640        dropout_21[0][0]                 \n__________________________________________________________________________________________________\nconv2d_88 (Conv2D)              (None, 2, 2, 16)     4624        conv2d_87[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_9 (UpSampling2D)  (None, 96, 96, 16)   0           conv2d_88[0][0]                  \n__________________________________________________________________________________________________\nconv2d_89 (Conv2D)              (None, 94, 94, 3)    435         up_sampling2d_9[0][0]            \n__________________________________________________________________________________________________\nflatten_19 (Flatten)            (None, 26508)        0           conv2d_89[0][0]                  \n__________________________________________________________________________________________________\nflatten_18 (Flatten)            (None, 1600)         0           conv2d_84[0][0]                  \n__________________________________________________________________________________________________\ndense_76 (Dense)                (None, 1024)         27145216    flatten_19[0][0]                 \n__________________________________________________________________________________________________\ndense_72 (Dense)                (None, 1024)         1639424     flatten_18[0][0]                 \n__________________________________________________________________________________________________\ndense_77 (Dense)                (None, 512)          524800      dense_76[0][0]                   \n__________________________________________________________________________________________________\ndense_73 (Dense)                (None, 512)          524800      dense_72[0][0]                   \n__________________________________________________________________________________________________\ndense_78 (Dense)                (None, 256)          131328      dense_77[0][0]                   \n__________________________________________________________________________________________________\ndense_74 (Dense)                (None, 256)          131328      dense_73[0][0]                   \n__________________________________________________________________________________________________\ndense_79 (Dense)                (None, 128)          32896       dense_78[0][0]                   \n__________________________________________________________________________________________________\nmobilenetv2_1.00_224 (Model)    (None, 3)            2261827     conv2d_89[0][0]                  \n__________________________________________________________________________________________________\ndense_75 (Dense)                (None, 128)          32896       dense_74[0][0]                   \n__________________________________________________________________________________________________\ndropout_22 (Dropout)            (None, 128)          0           dense_79[0][0]                   \n__________________________________________________________________________________________________\nclass_output (Dense)            (None, 3)            12          mobilenetv2_1.00_224[1][0]       \n__________________________________________________________________________________________________\npose_output (Dense)             (None, 7)            903         dense_75[0][0]                   \n__________________________________________________________________________________________________\nbbox_output (Dense)             (None, 4)            516         dropout_22[0][0]                 \n==================================================================================================\nTotal params: 32,483,381\nTrainable params: 32,449,269\nNon-trainable params: 34,112\n__________________________________________________________________________________________________\nEpoch 1/30\n600/600 [==============================] - 58s 96ms/step - loss: 10006.9773 - class_output_loss: 0.3124 - pose_output_loss: 4890.3018 - bbox_output_loss: 5116.3643 - class_output_categorical_accuracy: 0.5983 - pose_output_mean_squared_error: 4890.3018 - bbox_output_mean_squared_error: 5116.3643\nEpoch 2/30\n600/600 [==============================] - 42s 71ms/step - loss: 9239.3487 - class_output_loss: 0.1661 - pose_output_loss: 4166.0088 - bbox_output_loss: 5084.9302 - class_output_categorical_accuracy: 0.6233 - pose_output_mean_squared_error: 4166.0088 - bbox_output_mean_squared_error: 5084.9302\nEpoch 3/30\n600/600 [==============================] - 42s 70ms/step - loss: 8386.3081 - class_output_loss: 0.1616 - pose_output_loss: 3568.5396 - bbox_output_loss: 4825.9731 - class_output_categorical_accuracy: 0.6267 - pose_output_mean_squared_error: 3568.5396 - bbox_output_mean_squared_error: 4825.9731\nEpoch 4/30\n600/600 [==============================] - 45s 74ms/step - loss: 7863.2430 - class_output_loss: 0.1910 - pose_output_loss: 3178.1045 - bbox_output_loss: 4695.5244 - class_output_categorical_accuracy: 0.6167 - pose_output_mean_squared_error: 3178.1045 - bbox_output_mean_squared_error: 4695.5244\nEpoch 5/30\n600/600 [==============================] - 45s 76ms/step - loss: 7463.4890 - class_output_loss: 0.1631 - pose_output_loss: 2887.2559 - bbox_output_loss: 4586.4888 - class_output_categorical_accuracy: 0.6233 - pose_output_mean_squared_error: 2887.2559 - bbox_output_mean_squared_error: 4586.4888\nEpoch 6/30\n600/600 [==============================] - 45s 75ms/step - loss: 7282.3495 - class_output_loss: 0.1450 - pose_output_loss: 2698.9810 - bbox_output_loss: 4591.3115 - class_output_categorical_accuracy: 0.6283 - pose_output_mean_squared_error: 2698.9810 - bbox_output_mean_squared_error: 4591.3115\nEpoch 7/30\n600/600 [==============================] - 45s 75ms/step - loss: 7088.4800 - class_output_loss: 0.1774 - pose_output_loss: 2605.6504 - bbox_output_loss: 4500.2949 - class_output_categorical_accuracy: 0.6217 - pose_output_mean_squared_error: 2605.6504 - bbox_output_mean_squared_error: 4500.2949\nEpoch 8/30\n600/600 [==============================] - 44s 74ms/step - loss: 6930.2022 - class_output_loss: 0.1858 - pose_output_loss: 2545.0364 - bbox_output_loss: 4393.3755 - class_output_categorical_accuracy: 0.6200 - pose_output_mean_squared_error: 2545.0364 - bbox_output_mean_squared_error: 4393.3755\nEpoch 9/30\n600/600 [==============================] - 49s 82ms/step - loss: 6872.0121 - class_output_loss: 0.2031 - pose_output_loss: 2535.4287 - bbox_output_loss: 4345.5718 - class_output_categorical_accuracy: 0.6133 - pose_output_mean_squared_error: 2535.4287 - bbox_output_mean_squared_error: 4345.5718\nEpoch 10/30\n600/600 [==============================] - 49s 81ms/step - loss: 6851.8723 - class_output_loss: 0.1805 - pose_output_loss: 2496.6392 - bbox_output_loss: 4364.1704 - class_output_categorical_accuracy: 0.6200 - pose_output_mean_squared_error: 2496.6392 - bbox_output_mean_squared_error: 4364.1704\nEpoch 11/30\n600/600 [==============================] - 40s 67ms/step - loss: 6672.9842 - class_output_loss: 0.1666 - pose_output_loss: 2466.8672 - bbox_output_loss: 4217.5708 - class_output_categorical_accuracy: 0.6250 - pose_output_mean_squared_error: 2466.8672 - bbox_output_mean_squared_error: 4217.5708\nEpoch 12/30\n600/600 [==============================] - 49s 82ms/step - loss: 6633.5941 - class_output_loss: 0.1975 - pose_output_loss: 2410.0710 - bbox_output_loss: 4236.0850 - class_output_categorical_accuracy: 0.6150 - pose_output_mean_squared_error: 2410.0710 - bbox_output_mean_squared_error: 4236.0850\nEpoch 13/30\n600/600 [==============================] - 47s 78ms/step - loss: 6601.1972 - class_output_loss: 0.2097 - pose_output_loss: 2431.0188 - bbox_output_loss: 4179.4224 - class_output_categorical_accuracy: 0.6067 - pose_output_mean_squared_error: 2431.0188 - bbox_output_mean_squared_error: 4179.4224\nEpoch 14/30\n600/600 [==============================] - 47s 78ms/step - loss: 6522.3378 - class_output_loss: 0.1839 - pose_output_loss: 2404.4453 - bbox_output_loss: 4129.0596 - class_output_categorical_accuracy: 0.6183 - pose_output_mean_squared_error: 2404.4453 - bbox_output_mean_squared_error: 4129.0596\nEpoch 15/30\n600/600 [==============================] - 46s 76ms/step - loss: 6524.2282 - class_output_loss: 0.1833 - pose_output_loss: 2393.9438 - bbox_output_loss: 4142.2402 - class_output_categorical_accuracy: 0.6183 - pose_output_mean_squared_error: 2393.9438 - bbox_output_mean_squared_error: 4142.2402\nEpoch 16/30\n600/600 [==============================] - 46s 77ms/step - loss: 6504.1241 - class_output_loss: 0.2092 - pose_output_loss: 2377.3909 - bbox_output_loss: 4137.6494 - class_output_categorical_accuracy: 0.6067 - pose_output_mean_squared_error: 2377.3909 - bbox_output_mean_squared_error: 4137.6494\nEpoch 17/30\n600/600 [==============================] - 43s 71ms/step - loss: 6389.5349 - class_output_loss: 0.1969 - pose_output_loss: 2361.8967 - bbox_output_loss: 4036.4111 - class_output_categorical_accuracy: 0.6133 - pose_output_mean_squared_error: 2361.8967 - bbox_output_mean_squared_error: 4036.4111\nEpoch 18/30\n600/600 [==============================] - 43s 72ms/step - loss: 6405.1930 - class_output_loss: 0.1792 - pose_output_loss: 2356.8757 - bbox_output_loss: 4059.3354 - class_output_categorical_accuracy: 0.6183 - pose_output_mean_squared_error: 2356.8757 - bbox_output_mean_squared_error: 4059.3354\nEpoch 19/30\n600/600 [==============================] - 49s 82ms/step - loss: 6347.0819 - class_output_loss: 0.1637 - pose_output_loss: 2366.8604 - bbox_output_loss: 3988.9751 - class_output_categorical_accuracy: 0.6250 - pose_output_mean_squared_error: 2366.8604 - bbox_output_mean_squared_error: 3988.9751\nEpoch 20/30\n600/600 [==============================] - 43s 71ms/step - loss: 6337.4330 - class_output_loss: 0.2116 - pose_output_loss: 2348.8296 - bbox_output_loss: 3999.9934 - class_output_categorical_accuracy: 0.6033 - pose_output_mean_squared_error: 2348.8296 - bbox_output_mean_squared_error: 3999.9934\nEpoch 21/30\n600/600 [==============================] - 43s 72ms/step - loss: 6336.7198 - class_output_loss: 0.1838 - pose_output_loss: 2342.9595 - bbox_output_loss: 4003.9580 - class_output_categorical_accuracy: 0.6167 - pose_output_mean_squared_error: 2342.9595 - bbox_output_mean_squared_error: 4003.9580\nEpoch 22/30\n600/600 [==============================] - 43s 72ms/step - loss: 6333.5655 - class_output_loss: 0.1945 - pose_output_loss: 2353.8291 - bbox_output_loss: 3989.6421 - class_output_categorical_accuracy: 0.6133 - pose_output_mean_squared_error: 2353.8291 - bbox_output_mean_squared_error: 3989.6421\nEpoch 23/30\n600/600 [==============================] - 54s 90ms/step - loss: 6296.8819 - class_output_loss: 0.1756 - pose_output_loss: 2352.4395 - bbox_output_loss: 3953.4246 - class_output_categorical_accuracy: 0.6183 - pose_output_mean_squared_error: 2352.4395 - bbox_output_mean_squared_error: 3953.4246\nEpoch 24/30\n600/600 [==============================] - 43s 72ms/step - loss: 6357.0097 - class_output_loss: 0.2007 - pose_output_loss: 2390.5916 - bbox_output_loss: 3976.6904 - class_output_categorical_accuracy: 0.6117 - pose_output_mean_squared_error: 2390.5916 - bbox_output_mean_squared_error: 3976.6904\nEpoch 25/30\n600/600 [==============================] - 43s 72ms/step - loss: 6331.0764 - class_output_loss: 0.2010 - pose_output_loss: 2384.9773 - bbox_output_loss: 3957.6479 - class_output_categorical_accuracy: 0.6083 - pose_output_mean_squared_error: 2384.9773 - bbox_output_mean_squared_error: 3957.6479\nEpoch 26/30\n600/600 [==============================] - 43s 72ms/step - loss: 6367.3591 - class_output_loss: 0.1869 - pose_output_loss: 2461.4111 - bbox_output_loss: 3915.5054 - class_output_categorical_accuracy: 0.6167 - pose_output_mean_squared_error: 2461.4111 - bbox_output_mean_squared_error: 3915.5054\nEpoch 27/30\n600/600 [==============================] - 49s 82ms/step - loss: 6408.7652 - class_output_loss: 0.2242 - pose_output_loss: 2512.1509 - bbox_output_loss: 3904.2766 - class_output_categorical_accuracy: 0.6017 - pose_output_mean_squared_error: 2512.1509 - bbox_output_mean_squared_error: 3904.2766\nEpoch 28/30\n600/600 [==============================] - 45s 74ms/step - loss: 6376.5733 - class_output_loss: 0.1984 - pose_output_loss: 2472.1624 - bbox_output_loss: 3914.9058 - class_output_categorical_accuracy: 0.6083 - pose_output_mean_squared_error: 2472.1624 - bbox_output_mean_squared_error: 3914.9058\nEpoch 29/30\n600/600 [==============================] - 57s 95ms/step - loss: 6402.7925 - class_output_loss: 0.1901 - pose_output_loss: 2508.7925 - bbox_output_loss: 3901.8005 - class_output_categorical_accuracy: 0.6150 - pose_output_mean_squared_error: 2508.7925 - bbox_output_mean_squared_error: 3901.8005\nEpoch 30/30\n600/600 [==============================] - 62s 103ms/step - loss: 6360.8963 - class_output_loss: 0.2089 - pose_output_loss: 2508.9265 - bbox_output_loss: 3860.1733 - class_output_categorical_accuracy: 0.6067 - pose_output_mean_squared_error: 2508.9265 - bbox_output_mean_squared_error: 3860.1733\n600/600 [==============================] - 25s 42ms/step - loss: 6406.2124 - class_output_loss: 0.5832 - pose_output_loss: 2747.4299 - bbox_output_loss: 3658.1980 - class_output_categorical_accuracy: 0.3333 - pose_output_mean_squared_error: 2747.4299 - bbox_output_mean_squared_error: 3658.1980\n"
    },
    {
     "data": {
      "text/plain": "[6406.212383829753,\n 0.5832041,\n 2747.43,\n 3658.198,\n 0.33333334,\n 2747.43,\n 3658.198]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare for evaluation\n",
    "model = MobilePoseNet(output_classes)\n",
    "checkpoint_path = \"mobile_pose0035.h5\"\n",
    "model.load_weights(checkpoint_path)\n",
    "model.summary()\n",
    "#tf.keras.utils.plot_model(model, 'model.png', show_shapes=True, show_layer_names=True, dpi=600)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=optimizer, loss=[tf.keras.losses.CategoricalCrossentropy(), \n",
    "                                         tf.keras.losses.MeanSquaredError(), \n",
    "                                         tf.keras.losses.MeanSquaredError()], \n",
    "                                   metrics={'class_output': [tf.keras.metrics.CategoricalAccuracy()],\n",
    "                                            'pose_output': [tf.keras.metrics.MeanSquaredError()],\n",
    "                                            'bbox_output': [tf.keras.metrics.MeanSquaredError()]})\n",
    "\n",
    "model.evaluate(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFLite Conversion\n",
    "#converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "#model = converter.convert()\n",
    "#open('mobilePoseNet.tflite', 'wb').write(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}